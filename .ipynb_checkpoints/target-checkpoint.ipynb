{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tensorflow to classifying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, datasets, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tensorflow datasets, we are going to import the cifar10 data, and split in two types.\n",
    "The training data will be used to actually train the neural network, and the testing data will be used to evaluate the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, training_labels), (testing_data, testing_labels) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image data that comes from the cifar10 data is based in a 0-255 point scaling. Instead of doing like this, we're going to normalize the data in order to make the images matrix values in a range of 0-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = testing_data/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images that exist in the cifar10 dataset have a class. The images can be classified as cars, deers, airplanes, and more. The only problem here is the fact the class label in the dataset is not saved by it's name - they're categorized by number indexes. We will now create a list with the class labels ordered by the index of cifar10 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_names = ['Plane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Sheep', 'Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "for i in range(16): # Here i am creating a plot of 16 images in the dataset\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.imshow(training_data[i])\n",
    "    plt.xlabel(class_label_names[training_labels[i][0]]) #For each image, it's class label will be placed above\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note here that the images in the cifar10 dataset are compressed, and extremely reduced in size. They are 32x32 in pixels, which make them pretty suscetible to fail in classifying bloatful images.\n",
    "As this project is being made to test the tensorflow features, I personelly don't care about those specifications, since I am not planning to deploy this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to reduce the ammount of images for testing and training, since the dataset is enormous, and I have a normal computer, without a GPU to change to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm using a slice here to get only the first 20000 images from the cifar10 dataset\n",
    "training_data = training_data[:20000]\n",
    "training_labels = training_labels[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here too, reducing to 4000 images\n",
    "testing_data = testing_data[:4000]\n",
    "testing_labels = testing_labels[:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More then this could be good in a supercomputer. But on my computer, you know, I don't think is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential() # creating the model, the neural network will be an Convolutional one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first layer will be a 2D layer.\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape = (32, 32, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.MaxPooling2D(2, 2)) #currently studying what is this layer, to be sincere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Conv2D(64, (3, 3), activation='relu')) #another 2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.MaxPooling2D(2, 2)) # then another pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Conv2D(64, (3, 3), activation='relu')) # then another 2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten()) # now setting the data to be unidimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(64, activation='relu')) # adding a dense layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(10, activation='softmax')) # and by the end, a dense layer for softmax, to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now defined and modelled, so we need to fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 1.5016 - accuracy: 0.4534 - val_loss: 1.3342 - val_accuracy: 0.5220\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.2477 - accuracy: 0.5536 - val_loss: 1.1490 - val_accuracy: 0.5920\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 20s 33ms/step - loss: 1.0954 - accuracy: 0.6142 - val_loss: 1.0669 - val_accuracy: 0.6212\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.9947 - accuracy: 0.6481 - val_loss: 1.0449 - val_accuracy: 0.6380\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 20s 33ms/step - loss: 0.8936 - accuracy: 0.6856 - val_loss: 1.0975 - val_accuracy: 0.6252\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 20s 33ms/step - loss: 0.8168 - accuracy: 0.7142 - val_loss: 0.9818 - val_accuracy: 0.6560\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.7381 - accuracy: 0.7405 - val_loss: 0.9752 - val_accuracy: 0.6770\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.6624 - accuracy: 0.7697 - val_loss: 0.9839 - val_accuracy: 0.6762\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 20s 33ms/step - loss: 0.6044 - accuracy: 0.7882 - val_loss: 1.0402 - val_accuracy: 0.6718\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.5459 - accuracy: 0.8077 - val_loss: 1.0193 - val_accuracy: 0.6755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f966c459a90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_data, training_labels, epochs=10, validation_data=(testing_data, testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
